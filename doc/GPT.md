Generative Pre-trained Transformer：GPT
Generative Pre-trained + Transformer
预训练：不需要数据标注，使用人类已有数据直接训练AI
变形器：

大型语言模型时在自然语言处理（NLP）和自然语言生成（NLG）任务中利用深度学习的基础模型。为了帮助它们学习语言的复杂性和联系，大型语言模型在大量的数据上进行了预训练。
LLM本质上是一个基于Transformer的神经网络，由谷歌工程师在2017年一篇名为“Attention is All You Need”的文章中介绍。一个模型的先进性和性能可以通过它有多少个参数来判断。一个模型的参数是它在生成输出时考虑的因素数量。

本质更像一个概率论的算法，数据在计算时是一个多维的矢量矩阵，通过神经网络中的多个层进行大量计算后，得出每个文本的概率，然后输出最高概率的值。

* 算力
* 算法
* 数据

使用一种神经网络算法，即一种函数，将数据转化为具体向量矩阵，然后通过算力来进行矩阵计算，计算出值的概率，然后输出最大概率。
