Generative Pre-trained Transformer：GPT
Generative Pre-trained + Transformer
预训练：不需要数据标注，使用人类已有数据直接训练AI
变形器：

大型语言模型时在自然语言处理（NLP）和自然语言生成（NLG）任务中利用深度学习的基础模型。为了帮助它们学习语言的复杂性和联系，大型语言模型在大量的数据上进行了预训练。
LLM本质上是一个基于Transformer的神经网络，由谷歌工程师在2017年一篇名为“Attention is All You Need”的文章中介绍。一个模型的先进性和性能可以通过它有多少个参数来判断。一个模型的参数是它在生成输出时考虑的因素数量。

算力 算法 数据
